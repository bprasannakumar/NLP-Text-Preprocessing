{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization:\n",
    "The process of splitting an input document into meaningful chunks is called Tokenization and that chunk is called as a token. We can think of a token as a useful unit for further semantic processing. It can be a word, a sentence, a paragraph, or anything else. Let's look at few examples.\n",
    "\n",
    "There are various popular libraries available to do it like NLTK and Spacy.\n",
    "We will use NLTK and look at a few examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nltk.tokenize import WhitespaceTokenizer\n",
    "# from nltk.tokenize import WordPunctTokenizer\n",
    "# from nltk.tokenize import TreebankWordTokenizer\n",
    "# from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WhitespaceTokenizer: This tokenizer uses white space to split the document into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WhitespaceTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning', 'NLP', 'is', 'good,', \"isn't\", 'it?']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc1 = \"Learning NLP is good, isn't it?\"\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenizer.tokenize(doc1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WordPunctTokenizer: This tokenizer uses punctuation to split the document into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning', 'NLP', 'is', 'good', ',', 'isn', \"'\", 't', 'it', '?']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2 = \"Learning NLP is good, isn't it?\"\n",
    "tokenizer = WordPunctTokenizer()\n",
    "tokenizer.tokenize(doc2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TreebankWordTokenizer: This tokenizer uses word semantics to split the document into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning', 'NLP', 'is', 'good', ',', 'is', \"n't\", 'it', '?']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3 = \"Learning NLP is good, isn't it?\"\n",
    "tokenizer = TreebankWordTokenizer()\n",
    "tokenizer.tokenize(doc3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: Out the above three tokenizers, TreebankWordTokenizer splits the document using word semantics, which is quite appropriate.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RegexpTokenizer: This tokenizer uses custom regular expression to split the document into tokens. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comma_reg_exp = \"\\,\"\n",
    "# period_reg_exp = \"\\.\"\n",
    "# question_mark_reg_exp = \"\\?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Learning NLP is good', ' very good', ' very nice.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comma_reg_exp = \"\\,\"\n",
    "doc4 = \"Learning NLP is good, very good, very nice.\"\n",
    "tokenizer = RegexpTokenizer(comma_reg_exp, gaps=True)\n",
    "tokenizer.tokenize(doc4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc4 is splitted into three tokens using comma as a separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am leanring NLP', ' I am studying NLP', ' You should learn NLP']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "period_reg_exp = \"\\.\"\n",
    "doc5 = \"I am leanring NLP. I am studying NLP. You should learn NLP.\"\n",
    "tokenizer = RegexpTokenizer(period_reg_exp, gaps=True)\n",
    "tokenizer.tokenize(doc5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc5 is splitted into three tokens using period as a separator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Is learning NLP good', ' Yes, it is good.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question_mark_reg_exp = \"\\?\"\n",
    "doc6 = \"Is learning NLP good? Yes, it is good.\"\n",
    "tokenizer = RegexpTokenizer(question_mark_reg_exp, gaps=True)\n",
    "tokenizer.tokenize(doc6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doc6 is splitted into three tokens using question mark as a separator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Let's take a document corpus to perform tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(corpus, tokenizer):\n",
    "    corpus_tokens = []\n",
    "    for doc in corpus:\n",
    "        doc = doc.lower()\n",
    "        tokens = tokenizer.tokenize(doc)\n",
    "        corpus_tokens.append(tokens)\n",
    "    return corpus_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = ['This movie is good', 'That movie was bad', 'Not a good movie at all']\n",
    "tokenizer = TreebankWordTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = get_tokens(corpus, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'movie', 'is', 'good'], ['that', 'movie', 'was', 'bad'], ['not', 'a', 'good', 'movie', 'at', 'all']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the above function can be written using list comprehension as follows\n",
    "tokens_lc = [tokenizer.tokenize(doc.lower()) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['this', 'movie', 'is', 'good'], ['that', 'movie', 'was', 'bad'], ['not', 'a', 'good', 'movie', 'at', 'all']]\n"
     ]
    }
   ],
   "source": [
    "print(tokens_lc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Analysis:  The above output contains tokens for each document.  After performing tokenization, these tokens can be used further in the semantic analysis pipeline such as to perform stemming or lemmatization, checking if this is a stopword or not, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
