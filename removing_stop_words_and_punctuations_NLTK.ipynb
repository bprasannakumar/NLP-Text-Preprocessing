{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Words\n",
    "\n",
    "A stop word is a commonly used word such as \"a\", \"an\", \"the\" in English language. \n",
    "\n",
    "Even though they add meaning to a sentence while writing and speaking, they generally do not add any more information for semantic analysis for the model that we are creating. So, we need to remove those words and removing those words will largely decrease the vocabulary size and will help our model to train faster and also require less storage space. \n",
    "\n",
    "NLTK provides a list of stopwords and we can use that to remove those stopwords from our text while preprocessing the text corpus. We can also create our custom stopword list and use that to remove those stopwords from the text corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note:  We can add or delete words from the stop words list based on our requirement.  \n",
    "For example, if we are doing a sentiment analysis, we need to preserve the words like \"not\", \"is not\", \"need not\" etc. because the word \"not\" may change the meaning of the sentence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I love my dog\", \"You love your cat\", \"They love their birds\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(doc, tokenizer):\n",
    "    p_tokens = []\n",
    "    doc = doc.lower()\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    for token in tokens:\n",
    "        #check if token not in stop words list, then add to result\n",
    "        if token not in stop_words:\n",
    "            p_tokens.append(token)            \n",
    "    #convert the token list into a single string sentence\n",
    "    return \" \".join(p_tokens)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "corpus_processed = [preprocess(doc, tokenizer) for doc in corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['love dog', 'love cat', 'love birds']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you see from output, words like \"I\", \"my\", \"You\", \"your\", \"They\", \"their\" are removed.  In a large corpus, removing thses stop words will considerably reduce the vocabulary size and will decrease storage space requirement and training of the model may be faster since vocabular size is reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Punctuations and other special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, punctuations and other special characters do not add any more value to the text corpus and it would be wise to remove those. We will use regular expression to remove those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "PUNCTUATIONS_RE = '[.,?!:;]'\n",
    "SPECIAL_CHARS_RE = '[(),{},\\,/&%$#@*[\\]]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I (love) my dog, really?\", \"You [love] your {cat}!\", \"#They *love /their %birds$;\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punct(doc):   \n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(PUNCTUATIONS_RE, '', doc)\n",
    "    doc = re.sub(SPECIAL_CHARS_RE, '', doc)\n",
    "    return doc   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_processed = [remove_punct(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i love my dog really', 'you love your cat', 'they love their birds']"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As you seen in the output, all the punctuations and special characters are removed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's combine both the techniques and proprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(doc, tokenizer):\n",
    "    result = []\n",
    "    doc = doc.lower()\n",
    "    doc = re.sub(PUNCTUATIONS_RE, '', doc)\n",
    "    doc = re.sub(SPECIAL_CHARS_RE, '', doc)\n",
    "    tokens = tokenizer.tokenize(doc)\n",
    "    for token in tokens:\n",
    "        #check if token not in stop words list, then add to result\n",
    "        if token not in stop_words:\n",
    "            result.append(token)            \n",
    "    #convert the token list into a single string sentence\n",
    "    return \" \".join(result)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\"I (love) my dog, really?\", \"You [love] your {cat}!\", \"#They *love /their %birds$;\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = TreebankWordTokenizer()\n",
    "corpus_processed = [preprocess_text(doc, tokenizer) for doc in corpus] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['love dog really', 'love cat', 'love birds']\n"
     ]
    }
   ],
   "source": [
    "print(corpus_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you see in the output, all punctuations, special characters, and stop words are removed. This preprocessed text can be further used for semantic analysis.  Also, see how the vocabulary size is reduced after preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
